---
title: "p8105_hw5_sjz2120"
author: "Sophie Zhang (sjz2120)"
date: "2022-11-05"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(1)
```


# Problem 1
This zip file contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

Start with a dataframe containing all file names; the list.files function will help
Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.



# Problem 2

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository. Let's load in the raw dataset:

```{r homicides-raw-data}
wp_homicides_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides =
  read_csv(wp_homicides_url,
           col_types = c("c", "c", "c", "c", "c", "n", "c", "c", "c", "d", "d", "c")) %>% # Read in the raw csv dataset
  janitor::clean_names() %>%
  mutate(victim_age = as.numeric(victim_age),
         reported_date = as.character(reported_date),
         reported_date = as.Date(reported_date, "%Y%m%d")) # Change variable types for `reported_date` and `age`
```


### Describing the raw data

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

The raw `homicides` dataset from the Washington Post contains `r nrow(homicides)` rows/observations and `r ncol(homicides)` columns/variables. Each of the `r nrow(homicides)` rows represents a homicide. The `r ncol(homicides)` key variables in this dataset include: `r colnames(homicides)`:

* `uid` -- a unique ID for each observation
* `reported_date` -- the reported date of each homicide (yearmonthday)
* `victim_last` and `victim_first` -- the last and first name of each homicide victim
* Demographic information for each homicide victim, including their race (`victim_race`), age (`victim_age`), sex (`victim_sex`)
* The location of each homicide incident, including the `city` and `state` as well as the latitude and longitude (`lat` and `lon`)
* `disposition` -- the status of each homicide case (case closed/open and arrest status)


### Homicides numbers and unsolved homicides by city

Now, let's create a `city_state` variable combining the `city` and `state` variables (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

Below is a table showing the total number of homicides and the number of unsolved homicides in each city. These values were also saved to a new dataframe called `homicides_by_city`.

```{r homicides-by-city}
homicides = homicides %>%
  mutate(city_state = paste(city, state, sep = ", "))

homicides_by_city = homicides %>%
  group_by(city_state) %>%
  summarise(n_homicides = length(city_state),
            n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

homicides_city_table = homicides_by_city %>%
  knitr::kable(col.names = c('City, State', 'Total Homicides', 'Number Unsolved'))
  
homicides_city_table
```


### Estimating the proportion of unsolved homicides in Baltimore, MD

Below, I use the `prop.test` function to estimate the proportion of homicides that are unsolved in the city of Baltimore, MD. The output of prop.test is saved as a tidy dataframe called `unsolved_prop_balt`.

```{r estim-baltimore}
balt_unsolved = homicides_by_city %>% 
  filter(city_state == "Baltimore, MD") %>%
  pull(n_unsolved) # Number of unsolved homicides in Baltimore
balt_total = homicides_by_city %>% 
  filter(city_state == "Baltimore, MD") %>%
  pull(n_homicides) # Number of total homicides in Baltimore

unsolved_prop_balt = prop.test(x = balt_unsolved, n = balt_total) %>%
                broom::tidy()
```

From this new dataframe `unsolved_prop_balt`, we pull the values for the estimated proportion and 95% CI of unsolved homicides in Baltimore using inline R:

* **The estimated proportion of unsolved homicides in Baltimore, MD is `r unsolved_prop_balt %>% pull(estimate) %>% round(4)` (95% CI: `r unsolved_prop_balt %>% pull(conf.low) %>% round(4)`, `r unsolved_prop_balt %>% pull(conf.high) %>% round(4)`)**


### Estimating the proportion of unsolved homicides for all 50 cities

Let's run `prop.test` for each of the cities in the `homicides_by_city` dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Below, I do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, to create a tidy dataframe with estimated proportions and CIs for each city.

The resulting dataframe with estimated proportions and CIs of unsolved homicides for each city is saved as `unsolved_prop_all`.


```{r}
unsolved_prop_all = homicides_by_city %>% 
  mutate( 
    prop_test = map2(.x = n_unsolved, .y = n_homicides, ~prop.test(x = .x, n = .y)),
    estim_output = map(.x = prop_test, ~broom::tidy(.x))
    ) %>% 
  select(-prop_test) %>% 
  unnest(estim_output) %>%
  select(city_state, n_homicides, n_unsolved, estimate_prop = estimate, conf.low, conf.high)

unsolved_prop_all
```


### Plot of unsolved homicide estimates & CIs for major cities in the US
Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r plot-homicides-by-city}
homicides_city_plot = unsolved_prop_all %>%
  mutate(city_state = fct_reorder(city_state, estimate_prop)) %>%
  ggplot(aes(x = city_state, y = estimate_prop, color = city_state)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(title = "Estimated proportions (and 95% Confidence Intervals) of unsolved homicides, by city",
       subtitle = "For 50 major cities in the U.S.",
       x = "City (City, State)",
       y = "Estimated proportion of unsolved homicides (with 95% CI)") +
  theme(plot.title = element_text(size = 11, hjust = 0.5),
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        axis.text.x = element_text(size = 7, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 7, hjust = 0.5),
        legend.position = "none")

homicides_city_plot
```



# Problem 3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected --- put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. Let's conduct a simulation to explore power in a one-sample t-test.

### Simulation to explore power in a one-sample t-test

First we create a function `simulate()` to simulate one-sample t-tests (H:μ=0 using α=0.05) on a random normally distributed dataset (with n=30, σ=5, μ=0); and which outputs a tidied dataset containing the estimate and p-value.

We then use a `for` loop to generate 5000 datasets from the model and do a t-test for each of them. The outputs (estimate and p-value) of these 5000 t-tests are saved to a tidied table called `simulations`.

```{r simulation-t-test}
#Let's create a function called `simulate` that carries out a one-sample t-test on a random normal sample
simulate = function(n = 30, sigma = 5, mu = 0) {
  input =  rnorm(n, mean = mu, sd = sigma)
  t_test = t.test(input, conf.int = 0.95) %>%
    broom::tidy() %>%
    select(estimate, p.value)
  
  t_test
}

# Let's simulate the results of taking a t-test on a random normal sample with n=30, σ=5, μ=0, 5000 times
simulations = vector("list", 5000)
for (i in 1:5000) {
  simulations[[i]] = simulate()
}
simulations = simulations %>%
  bind_rows()
```

Now, let's create a new function `mu_simulate()` that will repeat the above 5000x simulation process for any μ (in this case, let's look at μ={0, 1, 2, 3, 4, 5, 6}):

```{r simulations-diffr-mu}
# Let's create a new function mu_simulate() and repeat this process 5000 times for a range of `mu_value`
mu_simulate = function(mu_value) {
  mu_sim = vector("list", 5000)
  for (i in 1:5000) {
    mu_sim[[i]] = simulate(mu = mu_value)
  }
  sim_output =
    mu_sim %>%
    bind_rows()
    
  sim_output
}

# Let's use the function mu_simulate() to simulate t-tests for 5000 different samples with μ={0, 1, 2, 3, 4, 5, 6}
mu_simulations =
  tibble(mu_value = c(0, 1, 2, 3, 4, 5, 6),
         estimate_table = map(mu_value, mu_simulate)) %>%
  unnest(estimate_table) %>%
  mutate(reject_null = ifelse(p.value < 0.05, TRUE, FALSE)) # Add a new variable `reject_null` conditioned on whether or not the null hypothesis is rejected (ie. when the `p.value` is < 0.05)

```


### Plot showing the power of the one-sample t-test
Below is a plot showing the proportion of times the null was rejected (the power of the test, in this case when the p-value < 0.05) on the y axis and the true value of μ on the x axis.

* **From the plot below, we see that as the true value of μ (the effect size) increases, the power of the test increases until it plateaus.**

```{r plot-showing-power}
# Let's create a dataframe called `test_power` based on `mu_simulations` in the previous code chunk, which contains the proportion of times the null was rejected (the power of the test) by mu_value
test_power = mu_simulations %>%
  group_by(mu_value) %>%
  summarise(reject_null_prop = sum(reject_null) / 5000)

# Now we can create a plot showing the power of the test at each mu value from μ={0, 1, 2, 3, 4, 5, 6}
test_power %>%
  ggplot(aes(x = mu_value, y = reject_null_prop)) +
  geom_point(alpha = 0.5, size = 3) +
  geom_smooth(alpha = 0.5) +
  labs(title = "The relationship between effect size and power for a one-sample t-test",
       subtitle = "Based on 5000 simulations for a random normal sample with μ={0, 1, 2, 3, 4, 5, 6}",
       x = "True population mean (μ)",
       y = "Proportion of times the null was rejected (Power)")  +
  theme(plot.title = element_text(size = 15, hjust = 0.5),
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))
```


### Plot showing the average estimate μ̂ vs the true value of μ

Below is a plot showing the average estimate of μ on the y axis and the true value of μ on the x axis, separated into two lines -- one for all samples and one for only those samples for which the null was rejected.

```{r}
# Let's create a dataframe called `test_power` based on `mu_simulations` in the previous code chunk, which contains the proportion of times the null was rejected (the power of the test) by mu_value
test_estimates_all = mu_simulations %>%
  group_by(mu_value) %>%
  summarise(avg_estim_all = mean(estimate)) %>%
  ggplot(aes(x = mu_value, y = avg_estim_all)) +
  geom_point() +
  geom_smooth()

test_estimates_reject = mu_simulations %>%
  group_by(mu_value) %>%
  filter(reject_null == TRUE) %>%
  summarise(avg_estim_reject = mean(estimate)) %>%
  ggplot(aes(x = mu_value, y = avg_estim_reject)) +
  geom_point() +
  geom_smooth()


test_estimates_all + test_estimates_reject
```

Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?