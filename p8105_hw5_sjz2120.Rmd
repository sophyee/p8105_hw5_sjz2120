---
title: "p8105_hw5_sjz2120"
author: "Sophie Zhang (sjz2120)"
date: "2022-11-05"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


# Problem 1
In the "data" folder in this HW5 R Project, there are 20 .csv files containing data from a longitudinal study that had a control arm and an experimental arm.

Below I've created a tidy data frame `exp_df` containing data from all participants in the 20 .csv files. To do so, I used the list.files function, and iterated over file names to read in data for each subject using purrr::map before saving the result as a new variable in the dataframe. I then tidied the data further, so the final cleaned `exp_df` dataframe includes the subject ID, arm, and observations over time.

### Creating a tidy dataframe for the experimental data

```{r Load-and-clean-P1-data, message=FALSE}
exp_df =
  tibble(
    files = list.files(path = "data/", full.names = TRUE)
  ) %>%
  mutate(data = purrr::map(.x = files, read_csv)) %>%
  unnest(data) %>%
  mutate(files = str_replace(files, "data//", ""),
         files = str_replace(files, ".csv", "")) %>%
  separate(files, into = c("arm", "subject_id"), sep = "_") %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation") %>%
  mutate(week = as.numeric(week)) %>%
  mutate(arm = str_replace(arm, "con", "control"),
         arm = str_replace(arm, "exp", "experimental"))

exp_df
```


### Spaghetti plot showing observations on each subject over time
Now, let's make a spaghetti plot showing observations on each subject over time. This will let us more easily observe differences between subjects in the control vs experimental group.

```{r spaghetti-plot-expdf}
exp_plot = exp_df %>%
  unite("arm_subject", c(arm, subject_id), sep = "_", remove = FALSE) %>%
  ggplot(aes(x = week, y = observation)) +
    geom_path(aes(color = arm, group = as.factor(arm_subject)), alpha = 0.5) +
    labs(title = "",
         subtitle = "Experimental Data Over Time",
         x = "Week",
         y = "Measurement")

exp_plot
```

From the spaghetti plot of experimental data over time, we see that there is indeed a noticeable difference between the control and experimental arms of the study. In particular, we notice that participants in the control arm seem to have on average lower measurements over time compared to participants in the experimental arm.


# Problem 2

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository. Let's load in the raw dataset:

```{r homicides-raw-data, message=FALSE}
wp_homicides_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides =
  read_csv(wp_homicides_url,
           col_types = c("c", "c", "c", "c", "c", "n", "c", "c", "c", "d", "d", "c")) %>% # Read in the raw csv dataset
  janitor::clean_names() %>%
  mutate(victim_age = as.numeric(victim_age),
         reported_date = as.character(reported_date),
         reported_date = as.Date(reported_date, "%Y%m%d")) # Change variable types for `reported_date` and `age`
```


### Describing the raw data

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

The raw `homicides` dataset from the Washington Post contains `r nrow(homicides)` rows/observations and `r ncol(homicides)` columns/variables. Each of the `r nrow(homicides)` rows represents a homicide. The `r ncol(homicides)` key variables in this dataset include: `r colnames(homicides)`:

* `uid` -- a unique ID for each observation
* `reported_date` -- the reported date of each homicide (yearmonthday)
* `victim_last` and `victim_first` -- the last and first name of each homicide victim
* Demographic information for each homicide victim, including their race (`victim_race`), age (`victim_age`), sex (`victim_sex`)
* The location of each homicide incident, including the `city` and `state` as well as the latitude and longitude (`lat` and `lon`)
* `disposition` -- the status of each homicide case (case closed/open and arrest status)


### Homicides numbers and unsolved homicides by city

Now, let's create a `city_state` variable combining the `city` and `state` variables (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

Below is a table showing the total number of homicides and the number of unsolved homicides in each city. These values were also saved to a new dataframe called `homicides_by_city`.

```{r homicides-by-city}
homicides = homicides %>%
  mutate(city_state = paste(city, state, sep = ", "))

homicides_by_city = homicides %>%
  group_by(city_state) %>%
  summarise(n_homicides = length(city_state),
            n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

homicides_city_table = homicides_by_city %>%
  knitr::kable(col.names = c('City, State', 'Total Homicides', 'Number Unsolved'))
  
homicides_city_table
```


### Estimating the proportion of unsolved homicides in Baltimore, MD

Below, I use the `prop.test` function to estimate the proportion of homicides that are unsolved in the city of Baltimore, MD. The output of prop.test is saved as a tidy dataframe called `unsolved_prop_balt`.

```{r estim-baltimore}
balt_unsolved = homicides_by_city %>% 
  filter(city_state == "Baltimore, MD") %>%
  pull(n_unsolved) # Number of unsolved homicides in Baltimore
balt_total = homicides_by_city %>% 
  filter(city_state == "Baltimore, MD") %>%
  pull(n_homicides) # Number of total homicides in Baltimore

unsolved_prop_balt = prop.test(x = balt_unsolved, n = balt_total) %>%
                broom::tidy()
```

From this new dataframe `unsolved_prop_balt`, we pull the values for the estimated proportion and 95% CI of unsolved homicides in Baltimore using inline R:

* **The estimated proportion of unsolved homicides in Baltimore, MD is `r unsolved_prop_balt %>% pull(estimate) %>% round(4)` (95% CI: `r unsolved_prop_balt %>% pull(conf.low) %>% round(4)`, `r unsolved_prop_balt %>% pull(conf.high) %>% round(4)`)**


### Estimating the proportion of unsolved homicides for all 50 cities

Let's run `prop.test` for each of the cities in the `homicides_by_city` dataset, and extract both the proportion of unsolved homicides and the confidence interval for each city. Below, I do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, to create a tidy dataframe with estimated proportions and CIs for each city.

The resulting dataframe with estimated proportions and CIs of unsolved homicides for each city is saved as `unsolved_prop_all`.


```{r unsolved-all-cities, message=FALSE}
unsolved_prop_all = homicides_by_city %>% 
  mutate( 
    prop_test = map2(.x = n_unsolved, .y = n_homicides, ~prop.test(x = .x, n = .y)),
    estim_output = map(.x = prop_test, ~broom::tidy(.x))
    ) %>% 
  select(-prop_test) %>% 
  unnest(estim_output) %>%
  select(city_state, n_homicides, n_unsolved, estimate_prop = estimate, conf.low, conf.high)

unsolved_prop_all
```


### Plot of unsolved homicide estimates & CIs for major cities in the US
Now let's create a plot `homicides_city_plot` that shows the estimates and CIs for each city. Cities are arranged according to the proportion of unsolved homicides (from lowest to highest).

```{r plot-homicides-by-city}
homicides_city_plot = unsolved_prop_all %>%
  mutate(city_state = fct_reorder(city_state, estimate_prop)) %>%
  ggplot(aes(x = city_state, y = estimate_prop, color = city_state)) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    labs(title = "Estimated proportions & 95% confidence intervals of unsolved homicides, by city",
          subtitle = "For 50 major cities in the U.S.",
          x = "City (City, State)",
          y = "Estimated proportion of unsolved homicides (w/95% CI)") +
    theme(plot.title = element_text(size = 11, hjust = 0.5),
          plot.subtitle = element_text(size = 10, hjust = 0.5),
          axis.text.x = element_text(size = 7, angle = 45, hjust = 1),
          axis.text.y = element_text(size = 7, hjust = 0.5),
          legend.position = "none")

homicides_city_plot
```



# Problem 3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected --- put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. Let's conduct a simulation to explore power in a one-sample t-test.

### Simulation to explore power in a one-sample t-test

First we create a function `simulate()` to simulate one-sample t-tests (H:μ=0 using α=0.05) on a random normally distributed dataset (with n=30, σ=5, μ=0); and which outputs a tidied dataset containing the estimate and p-value.

We then use a `for` loop to generate 5000 datasets from the model and do a t-test for each of them. The outputs (estimate and p-value) of these 5000 t-tests are saved to a tidied table called `simulations`.

```{r simulation-t-test}
#Let's create a function called `simulate` that carries out a one-sample t-test on a random normal sample
simulate = function(n = 30, sigma = 5, mu = 0) {
  input =  rnorm(n, mean = mu, sd = sigma)
  t_test = t.test(input, conf.int = 0.95) %>%
    broom::tidy() %>%
    select(estimate, p.value)
  
  t_test
}

# Let's simulate the results of taking a t-test on a random normal sample with n=30, σ=5, μ=0, 5000 times
simulations = vector("list", 5000)
for (i in 1:5000) {
  simulations[[i]] = simulate()
}
simulations = simulations %>%
  bind_rows()
```

Now, let's create a new function `mu_simulate()` that will repeat the above 5000x simulation process for any μ (in this case, let's look at μ={0, 1, 2, 3, 4, 5, 6}):

```{r simulations-diffr-mu}
# Let's create a new function mu_simulate() and repeat this process 5000 times for a range of `mu_value`
mu_simulate = function(mu_value) {
  mu_sim = vector("list", 5000)
  for (i in 1:5000) {
    mu_sim[[i]] = simulate(mu = mu_value)
  }
  sim_output =
    mu_sim %>%
    bind_rows()
    
  sim_output
}

# Let's use the function mu_simulate() to simulate t-tests for 5000 different samples with μ={0, 1, 2, 3, 4, 5, 6}
mu_simulations =
  tibble(mu_value = c(0, 1, 2, 3, 4, 5, 6),
         estimate_table = map(mu_value, mu_simulate)) %>%
  unnest(estimate_table) %>%
  mutate(reject_null = ifelse(p.value < 0.05, TRUE, FALSE)) # Add a new variable `reject_null` conditioned on whether or not the null hypothesis is rejected (ie. when the `p.value` is < 0.05)

```


### Plot illustrating the power of the one-sample t-test
Below is a plot called `test_power_plot` showing the proportion of times the null was rejected (the power of the test, in this case defined when the p-value < 0.05) on the y axis and the true value of μ on the x axis.

* **From the plot below, we see that as the true value of μ increases (corresponding to an increase in effect size), the power of the test increases as well (until it plateaus as it approaches 1). Therefore, the power of a test increases as the effect size increases.**

```{r plot-showing-power, message=FALSE}
# Let's create a dataframe called `test_power` based on `mu_simulations` in the previous code chunk, which contains the proportion of times the null was rejected (the power of the test) by mu_value
test_power = mu_simulations %>%
  group_by(mu_value) %>%
  summarise(reject_null_prop = sum(reject_null) / 5000)

# Now we can create a plot showing the power of the test at each mu value from μ={0, 1, 2, 3, 4, 5, 6}
test_power_plot = test_power %>%
  ggplot(aes(x = mu_value, y = reject_null_prop)) +
    geom_point(alpha = 0.5, size = 3) +
    geom_smooth(alpha = 0.5) +
    labs(title = "The relationship between effect size and power for a one-sample t-test",
        subtitle = "Based on 5000 simulations for a random normal sample with μ={0, 1, 2, 3, 4, 5, 6}",
        x = "True population mean (μ)",
        y = "Proportion of times the null was rejected (Power)")  +
    theme(plot.title = element_text(size = 15, hjust = 0.5),
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))

test_power_plot
```


### Plot showing the average estimate μ̂ vs the true value of μ

Below is a plot called `estim_true_plot` showing the average estimate of μ on the y axis and the true value of μ on the x axis, separated into two lines -- one for all samples and one for only those samples for which the null was rejected.

```{r plot-comparing-estim-true-means, message=FALSE}
# Let's start by creating two new dataframes `test_estimates_all` and `test_estimates_reject` summarizing the average estimates for the two different groups of samples
test_estimates_all = mu_simulations %>%
  group_by(mu_value) %>%
  summarise(avg_estim_all_samples = mean(estimate))

test_estimates_reject = mu_simulations %>%
  group_by(mu_value) %>%
  filter(reject_null == TRUE) %>%
  summarise(avg_estim_reject_null = mean(estimate))

# Let's combine the above two dataframes into `test_estimates_combine`, so we have the average estimates grouped by true value of mu all in one df, and in a pivot_longer format
test_estimates_combine =
  full_join(test_estimates_all, test_estimates_reject, by = "mu_value") %>%
  pivot_longer(
    avg_estim_all_samples:avg_estim_reject_null,
    names_to = "sample",
    names_prefix = "avg_estim_",
    values_to = "avg_estim")

# Now we can create a plot comparing the average estimate mean vs. the true population
estim_true_plot =
  test_estimates_combine %>%
  ggplot(aes(x = mu_value, y = avg_estim, group = sample)) +
    geom_point(aes(color = sample), alpha = 0.5) +
    geom_smooth(aes(color = sample), alpha = 0.5) +
    labs(title = "Comparing the average estimate mean (μ^) vs. the true population (μ)",
         subtitle = "Between all samples and samples where the null is rejected",
         x = "True population mean (μ)",
         y = "The average estimate of μ (μ^)") +
    theme(plot.title = element_text(size = 15, hjust = 0.5),
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))

estim_true_plot
```

#### Describing the plot above

* From the plot above, we see that the sample average of μ̂  across tests for which the null is rejected is not equal to the true value of μ from around μ = 0 to 4, and instead is larger compared to the true estimated mean (i.e. looking across all samples).
* However, when μ = 4 and higher, we see that the sample average of μ̂  across tests for which the null is rejected begins to approach/approximately equals the true value of μ.
  * This is because as the true value of μ (the effect size) increases, the power of the test also increases. Since the power of the test is the proportion of times the null is rejected, a lower power for smaller μ's would mean that the test rejects the null less for smaller μ's. This means that (looking at the average μ^ across tests where the null is rejected) we are taking the average across less samples when μ is small, meaning a greater error in the estimates.
  * This in turn leads to the observed difference between the sample average of μ̂  across tests for which the null is rejected vs. the true value of μ.